{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Interacting with [CodeGen](https://github.com/salesforce/CodeGen/)\n",
        "\n",
        "- Originally from afiaka87\n",
        "- Orginized Colab by mega b#6696\n",
        "- Modified by Penguin-jpg"
      ],
      "metadata": {
        "id": "UFoSzgxnvrlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Show gpu information\n",
        "from IPython.display import clear_output\n",
        "from google.colab.output import eval_js\n",
        "eval_js('google.colab.output.setIframeHeight(\"500\")')\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "5VtHpiGpqLWF",
        "outputId": "ee0f04cc-3941-42d3-8668-7147d9db76c1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr  7 02:25:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wPwrvvMlkTVA",
        "outputId": "ecd4dbf2-67c4-4492-bfe6-8a1965bc9eb6",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CodeGen'...\n",
            "remote: Enumerating objects: 99, done.\u001b[K\n",
            "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 99 (delta 45), reused 58 (delta 16), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (99/99), done.\n",
            "/content/CodeGen\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-62.0.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 30.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pip-22.0.4 setuptools-62.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.9.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 2041348096 bytes == 0x564b4b6a4000 @  0x7f53798e61e7 0x564b4886f407 0x564b4883917c 0x564b4891947a 0x564b4883bf9d 0x564b4892dd4d 0x564b488afec8 0x564b488aaa2e 0x564b4883d88a 0x564b488abb4f 0x564b488aaa2e 0x564b4883d88a 0x564b488abb4f 0x564b488aaa2e 0x564b4883d88a 0x564b488abb4f 0x564b488aaa2e 0x564b4883d88a 0x564b488abb4f 0x564b488aaa2e 0x564b4883d88a 0x564b488abb4f 0x564b4883d7aa 0x564b488abb4f 0x564b488aaa2e 0x564b4883d88a 0x564b488ac719 0x564b488aaa2e 0x564b4883d88a 0x564b488abb4f 0x564b488aaa2e\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 2551685120 bytes == 0x564bc516c000 @  0x7f53798e7615 0x564b4883917c 0x564b4891947a 0x564b4883bf9d 0x564b4892dd4d 0x564b488afec8 0x564b488aaa2e 0x564b4883d88a 0x564b488abb4f 0x564b488aaa2e 0x564b4883d88a 0x564b488abb4f 0x564b488aaa2e 0x564b4883d88a 0x564b488abb4f 0x564b488aaa2e 0x564b4883d88a 0x564b488abb4f 0x564b488aaa2e 0x564b4883d88a 0x564b488abb4f 0x564b4883d7aa 0x564b488abb4f 0x564b488aaa2e 0x564b4883d88a 0x564b488ac719 0x564b488aaa2e 0x564b4883d88a 0x564b488abb4f 0x564b488aaa2e 0x564b4883df21\n",
            "tcmalloc: large alloc 2041348096 bytes == 0x564b4b6a4000 @  0x7f53798e61e7 0x564b4886e338 0x564b48838ad7 0x564b4883aad0 0x564b4883bf9d 0x564b4892dd4d 0x564b488afec8 0x564b488aaa2e 0x564b4883d88a 0x564b488abb4f 0x564b488aaa2e 0x564b4883df21 0x564b4883e341 0x564b489ad882 0x564b4883c902 0x564b488b0522 0x564b4883d7aa 0x564b488afd30 0x564b488aaa2e 0x564b4883d88a 0x564b488afd30 0x564b488aaa2e 0x564b4883d88a 0x564b488ac719 0x564b4892eb76 0x564b488abd95 0x564b4892eb76 0x564b488abd95 0x564b4892eb76 0x564b488abd95 0x564b4883dce9\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m831.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.16.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0+cu111->-r requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (4.63.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.2/895.2 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2->-r requirements.txt (line 3)) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2->-r requirements.txt (line 3)) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2->-r requirements.txt (line 3)) (1.15.0)\n",
            "Installing collected packages: tokenizers, torch, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.9.0+cu111 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.0+cu111 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.9.0+cu111 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 torch-1.9.0+cu111 transformers-4.16.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#@markdown # Install requirements\n",
        "!git clone https://github.com/salesforce/CodeGen\n",
        "%cd CodeGen\n",
        "!pip install --upgrade pip setuptools\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Load model and tokenizer\n",
        "chosen_model = \"codegen-350M-mono\" #@param [\"codegen-350M-nl\", \"codegen-350M-multi\", \"codegen-350M-mono\", \"codegen-2B-nl\", \"codegen-2B-multi\", \"codegen-2B-mono\", \"codegen-6B-nl\", \"codegen-6B-multi\", \"codegen-6B-mono\", \"codegen-16B-nl\", \"codegen-16B-multi\", \"codegen-16B-mono\"]\n",
        "fp16 = True #@param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists(f'./checkpoints/{chosen_model}'):\n",
        "  !wget -P checkpoints https://storage.googleapis.com/sfr-codegen-research/checkpoints/{chosen_model}.tar.gz && tar -xvf checkpoints/{chosen_model}.tar.gz -C checkpoints/\n",
        "\n",
        "\n",
        "import torch\n",
        "from jaxformer.hf.sample import truncate as do_truncate\n",
        "from jaxformer.hf.sample import set_env, set_seed, print_time, create_model, create_custom_gpt2_tokenizer, create_tokenizer, sample\n",
        "\n",
        "# (0) constants\n",
        "\n",
        "models_nl = ['codegen-350M-nl', 'codegen-2B-nl', 'codegen-6B-nl', 'codegen-16B-nl']\n",
        "models_pl = ['codegen-350M-multi', 'codegen-2B-multi', 'codegen-6B-multi', 'codegen-16B-multi', 'codegen-350M-mono', 'codegen-2B-mono', 'codegen-6B-mono', 'codegen-16B-mono']\n",
        "models = models_nl + models_pl\n",
        "\n",
        "\n",
        "# (2) preamble\n",
        "\n",
        "set_env()\n",
        "\n",
        "pad = 50256\n",
        "device = torch.device('cuda:0')\n",
        "ckpt = f'./checkpoints/{chosen_model}'\n",
        "\n",
        "if device.type == \"cpu\":\n",
        "  print()\n",
        "  print(\"force full precision for cpu!!\")\n",
        "  print()\n",
        "  fp16 = False\n",
        "\n",
        "\n",
        "# (3) load\n",
        "\n",
        "with print_time('loading parameters'):\n",
        "  model = create_model(ckpt=ckpt, fp16=fp16).to(device)\n",
        "\n",
        "\n",
        "with print_time('loading tokenizer'):\n",
        "  if chosen_model in models_pl:\n",
        "    tokenizer = create_custom_gpt2_tokenizer()\n",
        "  else:\n",
        "    tokenizer = create_tokenizer()\n",
        "  tokenizer.padding_side = 'left'\n",
        "  tokenizer.pad_token = pad"
      ],
      "metadata": {
        "id": "xZbWFH1hkf9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "981cd64b-77c6-42f3-9094-8ff8498ae38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading parameters\n",
            "loading parameters took 17.28s\n",
            "loading tokenizer\n",
            "loading tokenizer took 1.16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Try out the model\n",
        "rng_seed = 42 #@param {type:\"integer\"}\n",
        "rng_deterministic = True #@param {type:\"boolean\"}\n",
        "p = 0.95 #@param {type:\"number\"}\n",
        "t = 0.2 #@param {type:\"number\"}\n",
        "max_length = 128 #@param {type:\"integer\"}\n",
        "batch_size = 1 #@param {type:\"integer\"}\n",
        "context = \"def hello_world():\" #@param {type:\"string\"}\n",
        "\n",
        "set_seed(rng_seed, deterministic=rng_deterministic)\n",
        "\n",
        "# (4) sample\n",
        "\n",
        "with print_time('sampling'):\n",
        "  completion = sample(device=device, model=model, tokenizer=tokenizer, context=context, pad_token_id=pad, num_return_sequences=batch_size, temp=t, top_p=p, max_length_sample=max_length)[0]\n",
        "  truncation = do_truncate(completion)\n",
        "\n",
        "  print('=' * 100)\n",
        "  print(completion)\n",
        "  print('=' * 100)\n",
        "  print(context+truncation)\n",
        "  print('=' * 100)\n",
        "    \n",
        "\n",
        "# !python -m jaxformer.hf.sample --model $chosen_model \\\n",
        "#                  --rng-seed $rng_seed \\\n",
        "#                  --p $p \\\n",
        "#                  --t $t \\\n",
        "#                  --max-length $max_length \\\n",
        "#                  --batch-size $batch_size \\\n",
        "#                  --context '$context'\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN2xY4xmkss0",
        "outputId": "5465330b-67b1-4647-f6d9-b43925a0b002",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sampling\n",
            "====================================================================================================\n",
            "\n",
            "    print(\"Hello World\")\n",
            "\n",
            "hello_world()\n",
            "\n",
            "#\n",
            "====================================================================================================\n",
            "def hello_world():\n",
            "    print(\"Hello World\")\n",
            "\n",
            "hello_world()\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "sampling took 1.28s\n"
          ]
        }
      ]
    }
  ]
}